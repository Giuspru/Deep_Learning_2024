{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proviamo a scrivere con i tensori, quello che avviene all'interno dei neuroni: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1., 1., 1.])\n",
      "tensor([0., 0., 0.])\n",
      "\n",
      "\n",
      "------------------------------------\n",
      "\n",
      "\n",
      "tensor([[ 0.4012, -1.2462, -0.8468],\n",
      "        [ 0.1535, -1.1417,  0.2827],\n",
      "        [-0.8094,  0.6223,  1.4280],\n",
      "        [-1.0686,  0.6513,  0.0415],\n",
      "        [ 1.1277, -1.3463,  1.0221]], requires_grad=True)\n",
      "torch.Size([5, 3])\n",
      "tensor([[-0.2145,  0.4940, -0.1844, -0.4558,  0.6861],\n",
      "        [-0.1431, -0.6246, -1.8327,  3.0185,  0.6079],\n",
      "        [-0.8951, -0.7414,  0.0236,  1.0189,  0.6565]], requires_grad=True)\n",
      "torch.Size([3, 5])\n",
      "\n",
      "\n",
      "------------------------------------\n",
      "\n",
      "\n",
      "tensor([ 0.2974, -0.4629,  1.1129], requires_grad=True)\n",
      "torch.Size([3])\n",
      "\n",
      "\n",
      "------------------------------------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch \n",
    "import numpy as np\n",
    "from Lezione_08102024 import spazio\n",
    "x = torch.ones(size = (5 , ))\n",
    "y = torch.zeros(size = (3 , ))\n",
    "print(x)\n",
    "print(y)\n",
    "spazio()\n",
    "\n",
    "'''\n",
    "we have an input x that is a vector of 5 elements.\n",
    "Also we have an output y that is a vector of 3 elements.\n",
    "Basically we can imagine, we have one sample with 5 features. We imagine also that the problem is multiclassification\n",
    "So we have 3 classes.\n",
    "I think, probabilly, the higer element of y identify the appartenance to the class.\n",
    "'''\n",
    "\n",
    "w = torch.randn(size = (5 , 3 ) , requires_grad=True)\n",
    "w1 = torch.randn(size = (3 , 5 ) , requires_grad=True)\n",
    "print(w)\n",
    "print(w.shape)\n",
    "print(w1)\n",
    "print(w1.shape) \n",
    "spazio()\n",
    "\n",
    "\n",
    "'''\n",
    "Piccola spiegazione sulle dimensioni di w e w1, il professore ha detto che la dimensione dovrà essere \n",
    "5x3, ma nelle NN dal punto di vista matematico ha senso fare 3x5. Questo perchè nei neuroni\n",
    "ci sarà il frutto dei risultati del prodotto W.x + b.\n",
    "'''\n",
    "\n",
    "b = torch.randn(size = (3 , ) , requires_grad=True) #tre sono i neuroni di output.\n",
    "print(b)\n",
    "print(b.shape)\n",
    "spazio()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.1018, -2.9235,  3.0404], grad_fn=<AddBackward0>)\n",
      "torch.Size([3])\n",
      "\n",
      "\n",
      "------------------------------------\n",
      "\n",
      "\n",
      "tensor([0.6228, 0.5630, 1.1754], grad_fn=<AddBackward0>)\n",
      "torch.Size([3])\n",
      "\n",
      "\n",
      "------------------------------------\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nQui divento pazzo, ovviamente queste due cose non sono la stessa cosa!\\nAnche se la dimensione è la stessa, non sono la stessa cosa!\\nva bene lo stesso??? Vedremo, io continuerò a implementarli insieme.\\n'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Implementiamo la moltimplicazione:\n",
    "z = torch.matmul(x, w) + b \n",
    "print(z)\n",
    "print(z.shape)\n",
    "spazio()\n",
    "z1= torch.matmul(w1, x) + b \n",
    "print(z1)\n",
    "print(z1.shape)\n",
    "spazio()\n",
    "'''\n",
    "Qui divento pazzo, ovviamente queste due cose non sono la stessa cosa!\n",
    "Anche se la dimensione è la stessa, non sono la stessa cosa!\n",
    "va bene lo stesso??? Vedremo, io continuerò a implementarli insieme.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.2949, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "\n",
      "\n",
      "------------------------------------\n",
      "\n",
      "\n",
      "tensor(1.1701, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "\n",
      "\n",
      "------------------------------------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Calcolo della Loss: \n",
    "loss = torch.nn.functional.binary_cross_entropy_with_logits(z , y)\n",
    "print(loss)\n",
    "spazio()\n",
    "loss1 = torch.nn.functional.binary_cross_entropy_with_logits(z1 , y)\n",
    "print(loss1)\n",
    "spazio()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A questo punto una volta che abbiamo la nostra loss function, quello che si fa è propagare il gradiente per aggiornare i parametri. Possiamo avere un piccolo aiutino per effettuare questa backpropagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradient function di z: <AddBackward0 object at 0x7f7cb920bc10>\n",
      "\n",
      "\n",
      "------------------------------------\n",
      "\n",
      "\n",
      "gradient function di z1: <AddBackward0 object at 0x7f7cb920bc10>\n",
      "\n",
      "\n",
      "------------------------------------\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nEseguendo lui ci dice che la funzione che ha generato z è una addbackward cioè una somma, poichè z di fatto è una somma tra matmul e il vettore b\\nA sua volta lui dirà che torch.matmul è una variabile che è generato come moltiplicazione tra matrici x e w. \\nPoi va su x e w, che sono la radice dell'albero di computazione e mi fermo.\\n\""
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pytorch pensa all'implementazione della backpropagation.\n",
    "#Noi implementiamo solo il forward.\n",
    "\n",
    "print(\"gradient function di z:\" , z.grad_fn )\n",
    "spazio()\n",
    "print(\"gradient function di z1:\" , z1.grad_fn )\n",
    "spazio()\n",
    "\n",
    "'''\n",
    "Eseguendo lui ci dice che la funzione che ha generato z è una addbackward cioè una somma, poichè z di fatto è una somma tra matmul e il vettore b\n",
    "A sua volta lui dirà che torch.matmul è una variabile che è generato come moltiplicazione tra matrici x e w. \n",
    "Poi va su x e w, che sono la radice dell'albero di computazione e mi fermo.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "------------------------------------\n",
      "\n",
      "\n",
      "tensor([[0.1751, 0.0170, 0.3181],\n",
      "        [0.1751, 0.0170, 0.3181],\n",
      "        [0.1751, 0.0170, 0.3181],\n",
      "        [0.1751, 0.0170, 0.3181],\n",
      "        [0.1751, 0.0170, 0.3181]])\n",
      "\n",
      "\n",
      "------------------------------------\n",
      "\n",
      "\n",
      "tensor([0.3921, 0.2294, 0.5728])\n",
      "\n",
      "\n",
      "------------------------------------\n",
      "\n",
      "\n",
      "tensor([[0.2169, 0.2169, 0.2169, 0.2169, 0.2169],\n",
      "        [0.2124, 0.2124, 0.2124, 0.2124, 0.2124],\n",
      "        [0.2547, 0.2547, 0.2547, 0.2547, 0.2547]])\n",
      "\n",
      "\n",
      "------------------------------------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Con la funzione Backword che viene iinvocata sulla loss, allora viene propagato il gradiente e viene applicato.\n",
    "loss.backward()\n",
    "spazio()\n",
    "loss1.backward()\n",
    "spazio()\n",
    "\n",
    "'''\n",
    "Quello che succede è che viene effettuata una propagazione all'indietro di tutto il gradiente, fino alla radice della catena di computazione,\n",
    "E salva i gradienti, sull'attributo grad di ogni tensore (solo sui tensori in cui ho imposto requires_grad=True).\n",
    "'''\n",
    "\n",
    "print(w.grad)\n",
    "spazio()\n",
    "print(b.grad)\n",
    "spazio()\n",
    "print(w1.grad)\n",
    "spazio()\n",
    "\n",
    "\n",
    "#ATTENZIONE: questo è il gradiente per ogni w_i. NON ABBIAMO ANCORA AGGIORNATO I PESI\n",
    "#Aggiornare i parametri è il compito di un ottimizzatore SGD, ADAM, ecc."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
