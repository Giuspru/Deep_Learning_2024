{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MAIN PROGETTO DI DEEP LEARNING:\n",
    "questo lo andremo ad analizzare punto per punto e louserò nel progetto...spero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd \n",
    "import os "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oggetto che modella il dataset di training e di test.\n",
    "Il dataset di tr e te, servirà ad avere un riferimento a dove si trovano le immagini o i dati in generale.\n",
    "Ci offrirà funzioni per attingere a queste immagini /dati "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-I dataset sono gli oggetti che ci farano perdere più tempo in fase di implementazione, perchè difficilmente si ricicla da un progetto ad un altro. \n",
    "-In questa eseritazione utilizzeremeo dei dataset già presenti in torchvision e ma in generale dovrebbe essere più complicato."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset = datasets.FashionMNIST(root='./data', train=True, download=True, transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = datasets.FashionMNIST(\n",
    "    root='./data',\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transforms.ToTensor()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-Dataloader: si preoccupa di campionare le batch dal dataset e di mischiare il dataset al termine di ogni epoca. ovviamente solo durante il training.\n",
    "-sono oggetti di pytorch, e li si importa da utils.data da pytorch.\n",
    "-vuole un riferimento al dataset.\n",
    "-la batch size, cioè quanti cmpioni deve campionare per creare la batch.\n",
    "-se deve shuffleare il dataset ogni volata che cambio epoca.\n",
    "\n",
    "per il test stessa identica cosa eccetto per qianto riguarda lo shuffle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(training_dataset, batch_size=64 , shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ci serve il modello ovviamente:\n",
    "non esiste la classe rete neurale, dovremo creare noi a mano la nostra rete.\n",
    "\n",
    "-si crea una classe neural network, che deve estendere una classe base di pytorch.\n",
    "-Module è una classe molto generica che modella layer reti neurali intere, è talmente generica che tutti i metodi la riconoscono.\n",
    "-La prima funzione da implementare è il Costruttore:\n",
    "    . Eredita il costruttore della classe Module\n",
    "    . Si crea la struttura della rete. Introducendo i vari strati che la compongono.\n",
    "        in questo caso avremo:\n",
    "        . flatterizzatore: prende un tensore di più dimensioni e lo flatterizza\n",
    "        . primo layer fc, i due parametri sono dimensionalità dell'input e nodi del layer a cui arrivo.\n",
    "        . layer relu\n",
    "        . secondo layer fc che prende 512 input(stesso output del primo) e sputa 10 output\n",
    "-secondoa funzione da implementare: forward per il processamento della rete da input a output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __int__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(28*28, 512) #input + quanti nodi ha quel layer\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(512, 10)\n",
    "    \n",
    "    def forward(self , X):\n",
    "        X  = self.flatten(X)\n",
    "        X = self.fc1(X)\n",
    "        X = self.relu(X)\n",
    "        X = self.fc2(X)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNetwork()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "batch_size = 64 #campioi selezionati ad ogni iterazione dell'ottimizzatore.\n",
    "epochs = 5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss funcion: \n",
    "-dipende dal task.\n",
    "-se vogliamo fare una classificazione possiamo utilizzare cross entropy loss\n",
    "-se vogliamo fare una regressione possiamo utilizzare MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ottimizzatore:\n",
    "-semplicemente li dobbiamo scegliere, dal sottopachetto .optim\n",
    "-vedremo che per come costruiremo il model(), questo avrà una funzione parameters che restituisce la lista di tutti i parametri della rete.\n",
    "-comunichiamo la lista, perchè nel loop ad ogni chiamata di optimizer.step() avverrà l'aggiornamento dei parametri mediante quello specifico ottimizzatore\n",
    "-lr passato come parmaetro che noi abbiamo già scelto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LOOP DI OTTIMIZZAZIONE: la struttura sarà verosimilmente molto simile a quella riportata nella cella sottostante. \n",
    "-> Primo loop sulle epoche.\n",
    "-> Secondo loop sulle batch. leggermente più complesso, perchè vengono utilizzate dei comandi paricolari:\n",
    "    - batch_indx mi dirà quante batch go processato fin'ora.\n",
    "    - (X , y) quello che mi deve restituire la batch dei campioni, ovvero features ed etichette.\n",
    "    - enumerate(train_dataloader) mi da qualcosa che è un iteratore, ovvero una lista di coppie (indice, valore(in questo caso una tupla)).\n",
    "-> Cosa succede nel secondo loop?\n",
    "    -chiedo di effettuare il forward, quindi la prediction del mio modello basandomi sui dati X\n",
    "    -calcolo la loss.\n",
    "    -Azzero i gradienti: operazione che va fatta perchè di default l'ottimizzatore accumula i gradienti tra più iterazioni\n",
    "    -loss.Backpropagation()\n",
    "    - Dopo aver calcolato tutti i gradienti lungo l'albero di calcolo, faccio fare uno step all'ottimizzatore (applica la formula adam per esempio, predi i valori dei parametri e li aggiorni con la tecnica che ti dico di usare.)\n",
    "\n",
    "-Terminato il secondo loop, ho terminato un'epoca. A questo punto il dataloader viene resettato, internamente il dataloader ha una funzione per mischiare i dati, inizia una nuova epoca, e ricomincia tutto e da capo.\n",
    "\n",
    "-Attenzione nessuno mi vieta al termine di ogni epoca di effettuare un test magari su di un validation set. Ricordiamoci però di usare il comando with torch.no_grad(): perchè non vogliamo che vengano calcolati i gradienti in fase di verifica (non serve)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in epochs:\n",
    "    for batch_indx , (X, y) in enumerate(train_dataloader):\n",
    "\n",
    "        prediction = model(X)\n",
    "        loss = loss_fn(prediction, y)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward() #Calcolo i gradienti \n",
    "        optimizer.step() #Modifico i parametri\n",
    "\n",
    "\n",
    "        if batch_indx % 100 == 0:\n",
    "            loss, current = loss.item(), batch_indx * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "    \n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_indx , (X, y) in enumerate(test_dataloader):\n",
    "\n",
    "            prediction = model(X) #SPIEGO SOTTO\n",
    "            #loss = loss_fn(prediction, y)\n",
    "            #test_loss += loss.item()\n",
    "            correct += (prediction.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    print(\"Accuracy: \", 100*correct/len(test_dataloader.dataset) , \"%\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spiegazione ultima parte: \n",
    "prediction essendo un porblema di classificazione mi fornisce delle probabilità per ognuna delle 3 possibili classi. A me interessa ovviamente la probabilità più alta, cioè quella che verosimilmente mi indica il risultato corretto.\n",
    "\n",
    "-prediction.argmax(1) estraggo il valore più grande di prediction.\n",
    "-faccio un check per vedere se il valore più grande è uguale al valore della classe.\n",
    "-se è uguale allora aumento il numero di predizioni corrette, convertendolo a float, sommando sulla batch e estraendo il valore\n",
    "\n",
    "Una volta fatto per tutto il dataset di test, me printo l'accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "COMMENTO SUI DATASET: \n",
    "Mentre il dataloader sarà essenzialmente sempre come lo abbiamo creato sopra, perchè è un oggetto che non pspecigfico del progetto, lui ha un riferimento a ds pesca i dati e costruisce le batch. A prescindere da come è fatto il dataset. \n",
    "\n",
    "Il dataset è specifico per ofgni task, quindi non ho un oggetto generale.\n",
    "Come faccio a creare un oggetto dataset custom? \n",
    "\n",
    "Sotto ci sarà un esempio:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torchvision.io import read_image\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self , annotation_file , img_dir , transform = None):\n",
    "        self.img_labels = pd.read_csv(annotation_file)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "    \n",
    "    def __getitem__(self , indx): #Devo tirare fuori un'immagine e un'etichetta concordi.\n",
    "        img_path = os.path.join(self.img_dir, self.img_labels.iloc[indx, 0]) #--> è il nome 0000.png dell'immagine\n",
    "        image = read_image(img_path)\n",
    "        label = self.img_labels.iloc[indx, 1]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image , label\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E' importnte rispettare delle regole:\n",
    "in particolare è importante rispettare che l'oggetto dataset che creiamo abbia un link con pytorch, dove questo link significa che la classe che andrò ad implementare e che rappresenta il mio dataset, deve ereditare una classe base di pytorch, che rappresenta un dataset genrrico. Classe astratta. Devo estendere una classe base di torch. \n",
    "\n",
    "-from torch.utils.data import Dataset\n",
    "-creiamo la classe che eredita informazioni dalla classe Dataset, che è messa tra parentesi.\n",
    "-creazione del costruttore: all'interno ci possono esssere diversi parametri questo è un esempio. Nel mio caso sto ipotizzando che il mio dataset sia una cartella di immagini persente nell'had_disk.\n",
    "    -img_dir è un path ad una cartella in cui sono presenti le vere immagini.\n",
    "    -annotation_file è un file in cui sono presenti tutte le etichette delle immagini. (img0001.png 0)\n",
    "    -transform è una funzione che trasforma i dati.\n",
    "-la funzione __len__(self) che deve ritornare sempre il numero di campioni del dataset. Come si calcola la lunghezza di questo dataset, è relativo al singolo problema. Questa informazione comunque è fondamentale, perchè questa informazione serve al dataloader per capire quante volte iterare sul dataset.\n",
    "\n",
    "Se annotation_file è di tipo CSV, 0000.png, 1 e così via per ogni riga.\n",
    "Se io ho questo file di annotazione e da un'altra parte ho una cartella con le immagini dove effettivamente c'è l'immagine 0000.png.\n",
    "Allora io potrei fare pandas read csv e leggerle, esattamente come viene fatto nel codice di spora.\n",
    "\n",
    "Quindi in questo caso self.img_labels è una lista lunga quante sono le mie immagini.\n",
    "\n",
    "- __getitem__ è la funzione che il dataloader chiama quando deve costruire le batches. Quinidi se io chiedo una batch di 64 elementi allora lui chiamerà 64 volte la funzione getitem sul dataset, e si fa dare ogni volta un sample cioè una coppia immagine etichetta.\n",
    "l'unica cosa che non mi è veramente chiara è l'indx che devo utilizzare. Però dato che questa funzione è chiamata dal dataloader, allora sarà lui a fornirgli l'index. \n",
    "Il dataloader fornisce un indx a caso nel range della len del dataset.\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
