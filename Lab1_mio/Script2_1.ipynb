{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proviamo a scrivere con i tensori l'espressione di quello che accade nei neuroni. (tralasciamo la non linearità):\n",
    "le prossime linee di codice rappresenterannno una rete neurale molto semplice, talmente semplice da non essere una vera rete neurale. \n",
    "In realtà è solo il primo strato. \n",
    "\n",
    "Prenderemo un vettore di input con 5 features.\n",
    "Un vettore di output con tre features.\n",
    "Un vettore bias, che si aggiunge a tutti i neuroni di output. \n",
    "\n",
    "qui aggiungici l'immagine di quello che sta accadentdo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1., 1., 1.])\n",
      "torch.Size([5])\n",
      "torch.Size([3])\n",
      "torch.Size([5, 3])\n",
      "torch.Size([5])\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import numpy as np \n",
    "\n",
    "x = torch.ones(size=(5,))\n",
    "y = torch.ones(size=(3,))\n",
    "w = torch.randn(size=(5,3), requires_grad=True)\n",
    "b = torch.randn(size=(3,), requires_grad=True) #Il bias è tre perchè i neuroni di output è tre \n",
    "\n",
    "print(x)\n",
    "print(x.shape)\n",
    "print(y.shape)\n",
    "print(w.shape)\n",
    "print(x.T.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Come abbiamo già visto l'operatore matmul effettua il dot product, ovvero il prodotto tra vettori/matrici.\n",
    "In questo momento sto moltiplicando un vettore 1x5 (vettore x), con la matrice 5x3 (matrice w).\n",
    "Il risultato è un vettore 1x3, esattamente come dovrebbe essere, ovvero ha le stesse dimensioni di y.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.1595, -1.5969, -0.0101], grad_fn=<AddBackward0>)\n",
      "torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "#Implementiamo la moltiplicazione: l'operatore matmul, effettua un prodotto matrice vettore.\n",
    "z = torch.matmul(x, w) + b\n",
    "print(z)\n",
    "print(z.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0320544242858887\n"
     ]
    }
   ],
   "source": [
    "#Calcolo di una loss: nn.functional è una sottolibreria di torch con dentro tutte le funzioni che vengono usate come loss.\n",
    "loss = torch.nn.functional.binary_cross_entropy_with_logits(z , y) #with logits, si aspetta che io non abbia utilizzato la sigmoide di attivaxionie, e la fa lui. \n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quello che dovrei fare a questo punto, e che viene fatto in generale durante qualsiasia addestramento di un modello, è l'implementazione del gradiente e quindi della backpropagation. \n",
    "\n",
    "In realtà l'implementazione del gradiente non deve essere fatta manualmente per forza, ma viene fatto automaticamente dalla libreria torch. \n",
    "\n",
    "Il framework fa la backpropagation, con il tool authomatic differentiation graph, che gli permette di costruirsi un grafo di computazione che tiene traccia delle variabili e delle operazioni fatte.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient function: <AddBackward0 object at 0x7fe32958eac0>\n",
      "Gradient: None\n"
     ]
    }
   ],
   "source": [
    "#Posso far vedere che un attributo dei tensori è proprio il gradiente: \n",
    "print(\"Gradient function:\" , z.grad_fn)\n",
    "print(\"Gradient:\" , w.grad) #E' ancora non calcolato, lo farà .backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<AddBackward0 object at 0x7fdaba04c1c0> mi sta dicendo: guarda che questa cosa è una somma in una certa zona di memmoria.\n",
    "E' effettivamente così, infatti z è un matmul() + b.\n",
    "Il processo continua all'indietro in maniera autonoma. \n",
    "\n",
    "\"AddBackward0\" serve nel momento in cui gli chiedo di calcolare i gradienti, in questo caso lui si dovrà ricordare di calcolare la derivata della funzione add. Così ricostruirà il gradiente. \n",
    "\n",
    "Come possiamo vedere dall'output, il gradiente non è stato ancora calcolato (None), invece sappiamo quale sarà la funzione che dovremmo utilizzare per calcolarlo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Come faccio a chiedere di calcolare il gradiente e di applicarlo? con la funzione backward().\n",
    "\n",
    "Una volta chiamata, fa una propagazione all'indietro di tutto il gradiente, fino alla radice del grafo computazionale,e salva i gradienti nell'attributo .grad di ogni tensore. \n",
    "\n",
    "Attenzione: lo fa solamente per i tensori per cui ho impostato requires_grad=True.\n",
    "Sono solo i tensori in cui richiedo che i parametri vengano aggiornati, per esempio il tensore di input è logico che non abbia requires_grad=True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1534, -0.2772, -0.1675],\n",
      "        [-0.1534, -0.2772, -0.1675],\n",
      "        [-0.1534, -0.2772, -0.1675],\n",
      "        [-0.1534, -0.2772, -0.1675],\n",
      "        [-0.1534, -0.2772, -0.1675]])\n",
      "\n",
      " tensor([-0.1534, -0.2772, -0.1675])\n",
      "\n",
      " None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/r5/hpz0k8h17tv0fycw73zl5vcr0000gn/T/ipykernel_1749/3360930092.py:4: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/build/aten/src/ATen/core/TensorBody.h:494.)\n",
      "  print(\"\\n\", z.grad)\n"
     ]
    }
   ],
   "source": [
    "#Posso printarmi i gradienti per esempio di w e b: \n",
    "print(w.grad)\n",
    "print(\"\\n\",b.grad)\n",
    "print(\"\\n\", z.grad)\n",
    "\n",
    "#NB:ancora non abbiamo aggiornato i parametri, abbiamo solamente calcolato il gradiente rispetto a w e b , e ce li siamo salvati nelle variabili w e b come loro attrivìbuti.\n",
    "#Non sono stati cambiati i valori, l'ottimizzatore che anvcora non abbiamo introdotto ha come compito di aggiornare i parametri.\n",
    "#L'ottimizzatore decide come usare i gradienti per aggiornare i parametri. (sgd, adam, rmsprop, etc...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Piccola appendice su come forzare la non richiesta del gradiente, sarà utile quando faremo fine tuning.\n",
    "\n",
    "z1 = torch.matmul(x, w) + b \n",
    "print(z1.requires_grad)\n",
    "\n",
    "---> True   questo perchè z combinazione di elementi che richiedono gradiente.\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    z2 = torch.matmul(x, w) + b\n",
    "    print(z2.requires_grad)\n",
    "\n",
    "---> False\n",
    "\n",
    "E' molot utile, per motivi di efficienza.\n",
    "Infatti durante l'addestramento noi richiediamo il gradiente, ma quando il modello va in fase di evaluation (magari alla fine di un'epoca vogliamo provare il modello sul validation swt), nonserve più la necessità di richiedere il gradiente.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
