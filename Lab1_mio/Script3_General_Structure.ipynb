{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spazio():\n",
    "    print(\"\\n\")\n",
    "    print(\"------------------------------------\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Costruiamo un loop di addestramento: partiamo dalla struttura del loop, perchè dopo andiamo a definire tutte le entità che vanno a definire l'addestramento di una nn con pytorch.\n",
    "STRUTTURA GENERALE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch \n",
    "from torchvision import datasets #modellazione dataset più famosi\n",
    "from torchvision import transforms \n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn\n",
    "#Oggetto che modella il dataset: servirà per accedere ai dati su disco: Project specific.\n",
    "\n",
    "training_dataset= datasets.FashionMNIST(\n",
    "    root=\"data\", #dove salvare i dati, si collega ad un URL remoto\n",
    "    train=True, #se è un dataset di training o di test\n",
    "    download=True, #se voglio scaricare i dati\n",
    "    transform=transforms.ToTensor(), \n",
    ")\n",
    "\n",
    "test_dataset = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transforms.ToTensor(),\n",
    ")\n",
    "\n",
    "\n",
    "'''\n",
    "Mentre il dataset servirà ad avere un riferimento a dove per esempio si trovano le immagini (path e directory)\n",
    "Vi offre delle funzioni per caricare delle immagini da disco ecc..\n",
    "\n",
    "Il dataloader si preoccupa di generare le batches, e quindi di campionare le batch dal dataset e poi di \n",
    "mischiare il dataset al termine di ogni epoca.\n",
    "\n",
    "'''\n",
    "#Strumento \n",
    "train_dataloader = DataLoader(training_dataset, batch_size=64, shuffle=True) \n",
    "test_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "\n",
    "#modello: la rete e la sua architettura, la dobbiamo creare noi.\n",
    "#Best practice è istanziare una classe che si estende da una classe base di pytorch.\n",
    "\n",
    "class NeuralNetwork(nn.Module): \n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__() #Costruttore della classe module che è quella base\n",
    "        \n",
    "        #dobbiamo istanziare i layers:\n",
    "        self.flatten = nn.Flatten() #Flatten è un layer che trasforma il tensore in un vettore. Prende img e crea vettore. \n",
    "        self.fc1 = nn.Linear(28*28, 512) #Dimensionalità dell'input è 28*28, 512 sono i neuroni nel primo hidden layer. NB: sono vettori.\n",
    "        self.relu = nn.ReLU() \n",
    "        self.fc2 = nn.Linear(512, 10) #Dimensione di iput è 512, 10 sono le classi.\n",
    "\n",
    "    #Adesso implemento la forward che è la funzione che spiega come la rete processa il dato fino all'output. Il Framework fa la backward.\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "model = NeuralNetwork()\n",
    "\n",
    "\n",
    "#Hyperparameters che serviranno per l'addestramento.\n",
    "learning_rate = 1e-3\n",
    "batch_size = 64\n",
    "epochs = 5\n",
    "\n",
    "\n",
    "#Loss function: che deve essere defintia. in realtà da scegliere e impende dal task.\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss() #per problemi di classificazione\n",
    "\n",
    "#Optimizer: che deve essere definito. in realtà scelto\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate) #per come costruiremo model avrà una funzione che si chiama paarameters() che restituisce la lista di tutti i parametri."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A questo punto abbiamo tutto l'occorrente per creare il nostro LOOP di addestramento che a grandi linee avrà sempre lo stesso schema:\n",
    "     - Iterazione per ogni Epoca.\n",
    "     - Per ogi Epoca:devo iterare per ogni batch.\n",
    "     - Per ogni batch:chiedo di fare il forward e calcolo la loss.Inoltre faccio la backpropagation, inizialmente azzerando i gradienti perchè di defaul l'ottimizzatore accumula i gradienti di più iterazioni. Poi calcolo la loss.backward() che dovrebbe servire per aggiornare i pesi. Infine ottimizzatore.step().\n",
    "     -terminata un'epoca, il dataloader viene resettato, e viengono rimischiati tutti i dati. \n",
    "     -ricomincia una nuova epoca, riparte tutta la storiella. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BLOCCO STANDARD\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for batch_indx, (X, y ) in enumerate(train_dataloader): #train_dataloader fornisce la tupla samples e labels. Enumerates mi riporta l'indice dell'iterazione.\n",
    "         \n",
    "         prediction = model.forward(X) #forward pass\n",
    "         loss = loss_fn(prediction, y) #calcolo loss\n",
    "\n",
    "         #backpropagation\n",
    "         optimizer.zero_grad()\n",
    "         loss.backward() #Calcolo i gradienti \n",
    "         optimizer.step() #Applica la formula per esempio di SGD. In pratica qui avviene la modifica dei parametri aggiornandoli con quei gradienti. w - alphaa*grad\n",
    "\n",
    "\n",
    "         if batch_indx % 10 == 0:\n",
    "            print(f\"loss: {loss.item()}\")\n",
    "\n",
    "    \n",
    "    '''\n",
    "    Nessuno ci vieta alla fine di ogni epoca di fare un test sul test set oppure su validation set\n",
    "    Lo si fa utilizzando la funzione with torch.no_grad(, poichè qua non abbiamo bisogno di calcolare i gradienti.\n",
    "    La sintassi dice che tutto quello che è nel blocco which ha no_grad settato.\n",
    "\n",
    "    '''\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_indx, (X, y) in enumerate(test_dataloader):\n",
    "            prediction = model(X)  #forward pass anche se non l'ho scritto model.forward(X)è un alias.\n",
    "            loss = loss_fn(prediction, y)\n",
    "            correct += (prediction.argmax(1) == y).type(torch.float).sum().item() #below\n",
    "            print(f\"loss: {loss.item()}\")\n",
    "\n",
    "    #Una volta fatto per tutto il dataset di accuracy possiamo printare la nostra accuracy:\n",
    "    print(f\"Accuracy: {100*(correct/len(test_dataset))}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "correct += (prediction.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "Siccome il problema è di classificazione, abbiamo delle probabilità per ogni classe. Ovvero le probabilità che quel campione appartenga a quella classe.\n",
    "Dobbiamo selezionare l'elemento tra le varie classi che ha la probabilità più alta.\n",
    "Se questo è uguale a y, cioè l'etichetta di supervisione associata, allora effettuo la somma e quindi ho classificato correttamente.\n",
    "ho forzato a tipo float e .item() mi fornisce il valore numerico."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ATTENZIONE: NELLO SCRIPT QUI SOPRA ABBIAMO CHE IL DATASET VIENE DIRETTAMENTE SCARICATO TRAMITE UN COMANDO DI PYTORCH. QUESTO PERCHE' IN PYTORCH SONO SALVATI I DATASETS PIU' FAMOSI. \n",
    "\n",
    "IN GENERALE QUESTO NON MI CAPITERà MAI! QUELLO CHE DOVRO' FARE SARA' COSTRUIRE UN OGGETTO DATASET CUSTOM.\n",
    "''' -E' impoertante che oggetto Datasset abbia una sorta dilink con pytorch, cioè che la classe che andiamo ad implementare che rappresenta il nostro dataset\n",
    "    deve ereditare una classe case di pytorch, che rappresente un dataset genrico, una classe astratta generica. Dobbiamo estendere una classe di base di torch.\n",
    "\n",
    "    -Ereditare da una classe mi da dei vincoli su cosa posso fare su questa nuova classe. \n",
    "    In particolare quando creiamo un nuovo oggetto che rappresenta il nostro Dataset, dobbiamo implementare almeno queste tre funzioni: init (costruttore)\n",
    "\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from torch.utils.data import Dataset #adesso posso creare la mia classe \n",
    "import os\n",
    "from torchvision.io import read_image\n",
    "\n",
    "'''\n",
    "Idealmente sto immaginando che il mio dataset sia una cartella di immagini, presente nel mio hard disk (sarà un path), e che associata a questa cartella \n",
    "vi sia un file di annotazioni (annotation file) nel quale sono presenti le etichette. \n",
    "\n",
    "img.001.jpg ---> etichetta \n",
    "ecc..\n",
    "\n",
    "E' possibile anche avere immagini in tre cartelle differenti, pertanto al costruttore dovrò passare 3 paths. Questa è la parte più custom\n",
    "\n",
    "Supponiamo per esempio che il nostro file annotation sia della forma csv:\n",
    "00001.jpg, 0\n",
    "00002.jpg, 1\n",
    "00003.jpg, 0\n",
    "00004.jpg, 2\n",
    "...\n",
    "\n",
    "'''\n",
    "\n",
    "class MyDataset(Dataset):                                             #Dataset è la classe di base da cui si eredità \n",
    "    def __init__(self , annotation_file , img_dir, transform = None):\n",
    "        self.annotation_lables = pd.read_csv(annotation_file)         # il file di annotazioni, ed è una lista lunga tanto quanti sono i numeri dei campioni.\n",
    "        self.img_dir = img_dir                                        # directory delle immagini\n",
    "        self.transform = transform                                    # trasformazioni da applicare\n",
    "\n",
    "    def __len__(self):                                                #ritorna la lunghezza del dataset, servirà al dataloader per iterare sul dataset.\n",
    "        return len(self.img_labels)                                   #ATTENZIONE: come si calcola questa lunghezza dipende dal caso specifico.\n",
    "    \n",
    "    def __getitem__(self, index): #E' la funzione che il dataloader chiama  quando deve costruire le batches. se batch size=64 allora questa funzione è chiamata 64 volte.\n",
    "        \n",
    "        #Devo tirare fuori una tupla (img.jpg, label)\n",
    "        img_path = os.join(self.img_dir, self.img_labels.iloc[index, 0]) #punto all'immagine in img_dir e gli assegno il nome che sta sul file annotation_lables\n",
    "\n",
    "        #Devo leggere immagine da disco\n",
    "        image = read_image(img_path)\n",
    "\n",
    "        #devo definire la lable:\n",
    "        label = self.img_labels.iloc[index, 1]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "    \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
